{"OVOD": {}, "OV": {}, "Prompt": {"2210.03117": "- 2022-10-06, **MaPLe: Multi-modal Prompt Learning**, Muhammad Uzair Khattak et.al., Paper: [http://arxiv.org/abs/2210.03117v1](http://arxiv.org/abs/2210.03117v1), Code: **[https://github.com/muzairkhattak/multimodal-prompt-learning](https://github.com/muzairkhattak/multimodal-prompt-learning)**\n", "2210.03114": "- 2022-10-06, **CLIP model is an Efficient Continual Learner**, Vishal Thengane et.al., Paper: [http://arxiv.org/abs/2210.03114v1](http://arxiv.org/abs/2210.03114v1), Code: **[https://github.com/vgthengane/continual-clip](https://github.com/vgthengane/continual-clip)**\n", "2210.03094": "- 2022-10-06, **VIMA: General Robot Manipulation with Multimodal Prompts**, Yunfan Jiang et.al., Paper: [http://arxiv.org/abs/2210.03094v1](http://arxiv.org/abs/2210.03094v1)\n", "2210.03057": "- 2022-10-06, **Language Models are Multilingual Chain-of-Thought Reasoners**, Freda Shi et.al., Paper: [http://arxiv.org/abs/2210.03057v1](http://arxiv.org/abs/2210.03057v1), Code: **[https://github.com/google-research/url-nlp](https://github.com/google-research/url-nlp)**\n", "2210.03029": "- 2022-10-06, **Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization**, Seonghyeon Ye et.al., Paper: [http://arxiv.org/abs/2210.03029v1](http://arxiv.org/abs/2210.03029v1), Code: **[https://github.com/seonghyeonye/rospr](https://github.com/seonghyeonye/rospr)**\n", "2210.02952": "- 2022-10-06, **Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation**, Xu Guo et.al., Paper: [http://arxiv.org/abs/2210.02952v1](http://arxiv.org/abs/2210.02952v1)\n", "2210.02902": "- 2022-10-06, **Spatial segregation of substitutional B atoms in graphene patterned by the moir\u00e9 superlattice on Ir(111)**, Marc G. Cuxart et.al., Paper: [http://arxiv.org/abs/2210.02902v1](http://arxiv.org/abs/2210.02902v1)\n", "2210.02875": "- 2022-10-06, **Binding Language Models in Symbolic Languages**, Zhoujun Cheng et.al., Paper: [http://arxiv.org/abs/2210.02875v1](http://arxiv.org/abs/2210.02875v1), Code: **[https://github.com/hkunlp/binder](https://github.com/hkunlp/binder)**\n", "2210.02768": "- 2022-10-06, **Distilling Task-specific Logical Rules from Large Pre-trained Models**, Tao Chen et.al., Paper: [http://arxiv.org/abs/2210.02768v1](http://arxiv.org/abs/2210.02768v1)\n", "2210.02706": "- 2022-10-06, **Rapid reconstruction of compact binary sources using meshfree approximation**, Lalit Pathak et.al., Paper: [http://arxiv.org/abs/2210.02706v1](http://arxiv.org/abs/2210.02706v1)\n", "2210.03730": "- 2022-10-07, **SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training**, Ziqiang Zhang et.al., Paper: [http://arxiv.org/abs/2210.03730v1](http://arxiv.org/abs/2210.03730v1), Code: **[https://github.com/microsoft/speecht5](https://github.com/microsoft/speecht5)**\n", "2210.03690": "- 2022-10-07, **Few-Shot Anaphora Resolution in Scientific Protocols via Mixtures of In-Context Experts**, Nghia T. Le et.al., Paper: [http://arxiv.org/abs/2210.03690v1](http://arxiv.org/abs/2210.03690v1)\n", "2210.03493": "- 2022-10-07, **Automatic Chain of Thought Prompting in Large Language Models**, Zhuosheng Zhang et.al., Paper: [http://arxiv.org/abs/2210.03493v1](http://arxiv.org/abs/2210.03493v1), Code: **[https://github.com/amazon-research/auto-cot](https://github.com/amazon-research/auto-cot)**\n", "2210.03350": "- 2022-10-07, **Measuring and Narrowing the Compositionality Gap in Language Models**, Ofir Press et.al., Paper: [http://arxiv.org/abs/2210.03350v1](http://arxiv.org/abs/2210.03350v1), Code: **[https://github.com/ofirpress/self-ask](https://github.com/ofirpress/self-ask)**\n", "2210.03347": "- 2022-10-07, **Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding**, Kenton Lee et.al., Paper: [http://arxiv.org/abs/2210.03347v1](http://arxiv.org/abs/2210.03347v1)\n", "2210.03337": "- 2022-10-07, **A Unified Framework for Multi-intent Spoken Language Understanding with prompting**, Feifan Song et.al., Paper: [http://arxiv.org/abs/2210.03337v1](http://arxiv.org/abs/2210.03337v1)\n", "2210.03304": "- 2022-10-07, **Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot ICD Coding**, Zhichao Yang et.al., Paper: [http://arxiv.org/abs/2210.03304v1](http://arxiv.org/abs/2210.03304v1), Code: **[https://github.com/whaleloops/KEPT](https://github.com/whaleloops/KEPT)**\n", "2210.03251": "- 2022-10-06, **Small Character Models Match Large Word Models for Autocomplete Under Memory Constraints**, Ganesh Jawahar et.al., Paper: [http://arxiv.org/abs/2210.03251v1](http://arxiv.org/abs/2210.03251v1)\n", "2210.03162": "- 2022-10-06, **Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models**, David Wingate et.al., Paper: [http://arxiv.org/abs/2210.03162v1](http://arxiv.org/abs/2210.03162v1)\n", "2210.04873": "- 2022-10-10, **CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation**, Tanay Dixit et.al., Paper: [http://arxiv.org/abs/2210.04873v1](http://arxiv.org/abs/2210.04873v1), Code: **[https://github.com/tanay2001/core](https://github.com/tanay2001/core)**\n", "2210.04845": "- 2022-10-10, **FS-DETR: Few-Shot DEtection TRansformer with prompting and without re-training**, Adrian Bulat et.al., Paper: [http://arxiv.org/abs/2210.04845v1](http://arxiv.org/abs/2210.04845v1)\n", "2210.04831": "- 2022-10-10, **Visual Prompt Tuning for Test-time Domain Adaptation**, Yunhe Gao et.al., Paper: [http://arxiv.org/abs/2210.04831v1](http://arxiv.org/abs/2210.04831v1)\n", "2210.04726": "- 2022-10-10, **Knowledge Prompts: Injecting World Knowledge into Language Models through Soft Prompts**, Cicero Nogueira dos Santos et.al., Paper: [http://arxiv.org/abs/2210.04726v1](http://arxiv.org/abs/2210.04726v1)\n", "2210.04695": "- 2022-10-10, **Language Models Are Poor Learners of Directional Inference**, Tianyi Li et.al., Paper: [http://arxiv.org/abs/2210.04695v1](http://arxiv.org/abs/2210.04695v1), Code: **[https://github.com/teddy-li/lm-dirctionalinference](https://github.com/teddy-li/lm-dirctionalinference)**\n", "2210.04457": "- 2022-10-10, **XPrompt: Exploring the Extreme of Prompt Tuning**, Fang Ma et.al., Paper: [http://arxiv.org/abs/2210.04457v1](http://arxiv.org/abs/2210.04457v1)\n", "2210.04356": "- 2022-10-09, **Recent ALICE results on quarkonium production in nuclear collisions**, Biswarup Paul et.al., Paper: [http://arxiv.org/abs/2210.04356v1](http://arxiv.org/abs/2210.04356v1)\n", "2210.04325": "- 2022-10-11, **ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models**, Jiannan Xiang et.al., Paper: [http://arxiv.org/abs/2210.04325v2](http://arxiv.org/abs/2210.04325v2), Code: **[https://github.com/szxiangjn/any-shot-data2text](https://github.com/szxiangjn/any-shot-data2text)**\n", "2210.04287": "- 2022-10-09, **Learning to Decompose Visual Features with Latent Textual Prompts**, Feng Wang et.al., Paper: [http://arxiv.org/abs/2210.04287v1](http://arxiv.org/abs/2210.04287v1)\n", "2210.04238": "- 2022-10-09, **On some features of the solar proton event on 2021 October 28 (GLE73)**, I. M. Chertok et.al., Paper: [http://arxiv.org/abs/2210.04238v1](http://arxiv.org/abs/2210.04238v1)\n", "2210.05662": "- 2022-10-11, **Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems**, Zhengbang Zhu et.al., Paper: [http://arxiv.org/abs/2210.05662v1](http://arxiv.org/abs/2210.05662v1)\n", "2210.05643": "- 2022-10-11, **A Kernel-Based View of Language Model Fine-Tuning**, Sadhika Malladi et.al., Paper: [http://arxiv.org/abs/2210.05643v1](http://arxiv.org/abs/2210.05643v1), Code: **[https://github.com/princeton-nlp/lm-kernel-ft](https://github.com/princeton-nlp/lm-kernel-ft)**\n", "2210.05257": "- 2022-10-11, **Rethinking the Event Coding Pipeline with Prompt Entailment**, Cl\u00e9ment Lefebvre et.al., Paper: [http://arxiv.org/abs/2210.05257v1](http://arxiv.org/abs/2210.05257v1), Code: **[https://github.com/clement-lef/pr-ent](https://github.com/clement-lef/pr-ent)**\n", "2210.05159": "- 2022-10-11, **Can Language Models Be Specific? How?**, Jie Huang et.al., Paper: [http://arxiv.org/abs/2210.05159v1](http://arxiv.org/abs/2210.05159v1)\n", "2210.04982": "- 2022-10-10, **REV: Information-Theoretic Evaluation of Free-Text Rationales**, Hanjie Chen et.al., Paper: [http://arxiv.org/abs/2210.04982v1](http://arxiv.org/abs/2210.04982v1)\n", "2210.06466": "- 2022-10-12, **Prompt Generation Networks for Efficient Adaptation of Frozen Vision Transformers**, Jochem Loedeman et.al., Paper: [http://arxiv.org/abs/2210.06466v1](http://arxiv.org/abs/2210.06466v1), Code: **[https://github.com/jochemloedeman/pgn](https://github.com/jochemloedeman/pgn)**\n", "2210.06376": "- 2022-10-12, **Probing Commonsense Knowledge in Pre-trained Language Models with Sense-level Precision and Expanded Vocabulary**, Daniel Loureiro et.al., Paper: [http://arxiv.org/abs/2210.06376v1](http://arxiv.org/abs/2210.06376v1), Code: **[https://github.com/danlou/synbert](https://github.com/danlou/synbert)**\n", "2210.06349": "- 2022-10-12, **Context Generation Improves Open Domain Question Answering**, Dan Su et.al., Paper: [http://arxiv.org/abs/2210.06349v1](http://arxiv.org/abs/2210.06349v1)\n", "2210.06284": "- 2022-10-12, **Visual Prompting for Adversarial Robustness**, Aochuan Chen et.al., Paper: [http://arxiv.org/abs/2210.06284v1](http://arxiv.org/abs/2210.06284v1)\n", "2210.06229": "- 2022-10-12, **Towards visually prompted keyword localisation for zero-resource spoken languages**, Leanne Nortje et.al., Paper: [http://arxiv.org/abs/2210.06229v1](http://arxiv.org/abs/2210.06229v1)\n", "2210.05901": "- 2022-10-12, **Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning**, Hui-Chi Kuo et.al., Paper: [http://arxiv.org/abs/2210.05901v1](http://arxiv.org/abs/2210.05901v1)\n", "2210.05836": "- 2022-10-11, **CLIP also Understands Text: Prompting CLIP for Phrase Understanding**, An Yan et.al., Paper: [http://arxiv.org/abs/2210.05836v1](http://arxiv.org/abs/2210.05836v1)\n", "2210.07228": "- 2022-10-13, **Language Model Decoding as Likelihood-Utility Alignment**, Martin Josifoski et.al., Paper: [http://arxiv.org/abs/2210.07228v1](http://arxiv.org/abs/2210.07228v1)\n", "2210.07225": "- 2022-10-13, **Unified Vision and Language Prompt Learning**, Yuhang Zang et.al., Paper: [http://arxiv.org/abs/2210.07225v1](http://arxiv.org/abs/2210.07225v1), Code: **[https://github.com/yuhangzang/upt](https://github.com/yuhangzang/upt)**\n", "2210.07179": "- 2022-10-13, **MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting**, Oscar Ma\u00f1as et.al., Paper: [http://arxiv.org/abs/2210.07179v1](http://arxiv.org/abs/2210.07179v1)\n", "2210.07120": "- 2022-10-13, **Patterns of Structural Reflection in the large-cardinal hierarchy**, Joan Bagaria et.al., Paper: [http://arxiv.org/abs/2210.07120v1](http://arxiv.org/abs/2210.07120v1)\n", "2210.07054": "- 2022-10-13, **Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation**, Jinhui Ye et.al., Paper: [http://arxiv.org/abs/2210.07054v1](http://arxiv.org/abs/2210.07054v1), Code: **[https://github.com/atrewin/pgen](https://github.com/atrewin/pgen)**\n", "2210.07032": "- 2022-10-13, **Prompt-based Connective Prediction Method for Fine-grained Implicit Discourse Relation Recognition**, Hao Zhou et.al., Paper: [http://arxiv.org/abs/2210.07032v1](http://arxiv.org/abs/2210.07032v1), Code: **[https://github.com/zh-i9/pcp-for-idrr](https://github.com/zh-i9/pcp-for-idrr)**\n", "2210.06998": "- 2022-10-13, **DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Diffusion Models**, Zeyang Sha et.al., Paper: [http://arxiv.org/abs/2210.06998v1](http://arxiv.org/abs/2210.06998v1)\n", "2210.06908": "- 2022-10-13, **Feature-Proxy Transformer for Few-Shot Segmentation**, Jian-Wei Zhang et.al., Paper: [http://arxiv.org/abs/2210.06908v1](http://arxiv.org/abs/2210.06908v1), Code: **[https://github.com/jarvis73/fptrans](https://github.com/jarvis73/fptrans)**\n", "2210.06774": "- 2022-10-13, **Re3: Generating Longer Stories With Recursive Reprompting and Revision**, Kevin Yang et.al., Paper: [http://arxiv.org/abs/2210.06774v1](http://arxiv.org/abs/2210.06774v1), Code: **[https://github.com/yangkevin2/emnlp22-re3-story-generation](https://github.com/yangkevin2/emnlp22-re3-story-generation)**\n", "2210.06726": "- 2022-10-13, **Explanations from Large Language Models Make Small Reasoners Better**, Shiyang Li et.al., Paper: [http://arxiv.org/abs/2210.06726v1](http://arxiv.org/abs/2210.06726v1)\n"}, "OpenVocabularyOD": {}, "OpenVocabulary": {}}